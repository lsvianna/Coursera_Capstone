{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid_register_v2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyhSdSfVf7XEU1SaYlkmk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsvianna/Coursera_Capstone/blob/master/src/covid_register_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv_ERHY_ZvFx"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Jan 29 09:45:19 2021\n",
        "\n",
        "@author: viannaLS\n",
        "\n",
        "Programa para executar a‌ ‌predição‌ ‌da‌ ‌incidência‌ ‌diária‌ ‌de‌ ‌COVID-19‌ nos‌\n",
        "municípios‌ ‌de‌ ‌Santa‌ Catarina,‌ ‌através‌ ‌da‌ ‌execução‌ ‌de‌ ‌uma‌ ‌modelagem‌ ‌de‌ ‌dados‌ com‌\n",
        "um‌ ‌algoritmo‌ ‌de‌ aprendizagem‌ ‌de‌ ‌máquina. Uma‌ ‌rede‌ ‌neural‌ ‌recorrente‌ ‌foi‌\n",
        "contruída ‌para‌ ‌modelar‌ ‌um‌ ‌problema‌ ‌de‌ ‌regressão‌ ‌com‌ propósito‌ ‌preditivo,‌ ‌através‌\n",
        "de‌ ‌um‌ ‌estudo‌ epidemiológico‌ ‌longitudinal‌ ‌retrospectivo‌ ‌da‌ incidência‌ ‌de‌ ‌COVID-19‌\n",
        "nos‌ municípios‌ ‌analisados.‌ ‌A‌ ‌métrica‌ ‌de‌ ‌avaliação‌ ‌RMSE‌ ‌foi‌ utilizada‌ ‌para‌\n",
        "avaliar‌ os‌ ‌modelos‌ ‌obtidos.‌\n",
        "\n",
        "INPUT:\n",
        "start_date: data determinada para início da série temporal\n",
        "end_date: data determinada para fim da série temporal\n",
        "city: cidade para avaliação individual\n",
        "file: arquivo com conjunto de dados\n",
        "file_mun: arquivo com relação de municípios\n",
        "steps_in: quantidade de dias de entrada para modelagem\n",
        "steps_out: horizonte de predição\n",
        "split_size: fração dos dados de treino\n",
        "epochs: épocas do modelo\n",
        "batch: batches do modelo:\n",
        "nodes: quantidade de neurônios de referência\n",
        "\n",
        "OUTPUT:\n",
        "RMSE do modelo construído\n",
        "\n",
        "FILES:\n",
        "acf.jpg: Gráfico de autocorrelação total\n",
        "error.jpg: Gráfico erro em cada dia do horizonte de predição\n",
        "error_metric.csv: Erro em cada dia do horizonte de predição\n",
        "horizon.jpg: Gráficos com amostras nos diferentes horizontes de predição\n",
        "incidence.jpg: Gráfico com os dados da incidência e a média móvel\n",
        "pacf.jpg: Gráfico de autocorrelação parcial\n",
        "predictions.csv: Predições do modelo\n",
        "predictions.jpg: Gráfico com os dados da incidência, predições e suas médias\n",
        "móveis\n",
        "predictions_city.csv: Predições do modelo na cidade individualizada\n",
        "predictions_city.jpg: Gráfico com os dados da incidência, predições e suas\n",
        "médias móveis, na cidade individualizada\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.layers import RepeatVector, TimeDistributed\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "def pivot_table(data, pivot_arg):\n",
        "    index, columns, values, sel_condition = pivot_arg\n",
        "    data_condition = data[data[values] == sel_condition]\n",
        "    table_case = data_condition.pivot_table(index = index,\n",
        "                                            columns = columns,\n",
        "                                            values = values,\n",
        "                                            aggfunc = len,\n",
        "                                            fill_value = 0)\n",
        "    date_min = table_case.index.min()\n",
        "    date_max = table_case.index.max()\n",
        "    datas = pd.date_range(start = date_min, end = date_max)\n",
        "    table_case = table_case.reindex(datas, fill_value = 0)\n",
        "    return table_case\n",
        "\n",
        "def train_test_split(data, percent):\n",
        "    split = int(len(data.index) * percent)\n",
        "    train = data[:split]\n",
        "    test = data[split:]\n",
        "    return train, test\n",
        "\n",
        "def split_sequences(data, steps_in, steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(data)):\n",
        "        end_ix = i + steps_in\n",
        "        out_end_ix = end_ix + steps_out\n",
        "        if out_end_ix > len(data):\n",
        "            break\n",
        "        seq_x = data[i:end_ix, :]\n",
        "        seq_y = data[end_ix:out_end_ix, :]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_model(X, y, model_arg):\n",
        "    epochs, batch, features, steps_in, steps_out, nodes = model_arg\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(nodes, activation = 'relu',\n",
        "                   input_shape = (steps_in, features),\n",
        "                   kernel_initializer = 'glorot_normal',\n",
        "                   recurrent_initializer = 'glorot_normal',\n",
        "                   bias_initializer = 'random_normal',\n",
        "                   kernel_regularizer = 'l2'))\n",
        "    model.add(RepeatVector(steps_out))\n",
        "    model.add(LSTM(nodes, activation='relu', return_sequences=True,\n",
        "                    kernel_initializer = 'glorot_normal',\n",
        "                    recurrent_initializer = 'glorot_normal',\n",
        "                    bias_initializer = 'random_normal',\n",
        "                    kernel_regularizer = 'l2'))\n",
        "    model.add(TimeDistributed(Dense((int(nodes / 2)), activation = 'relu',\n",
        "                                    kernel_initializer = 'glorot_normal',\n",
        "                                    bias_initializer = 'random_normal',\n",
        "                                    kernel_regularizer = 'l2')))\n",
        "    model.add(TimeDistributed(Dense(features, activation = 'relu',\n",
        "                                    kernel_initializer = 'glorot_normal',\n",
        "                                    bias_initializer = 'random_normal',\n",
        "                                    kernel_regularizer = 'l2')))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    model.fit(X, y, epochs = epochs, batch_size = batch, verbose = False)  \n",
        "    return model\n",
        "\n",
        "# Define o valor da semente nos diferentes ambientes aplicados\n",
        "seed_value= 12345\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "# Configura sessão padrão do Keras\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                        inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(),\n",
        "                            config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "# Variáveis de entrada\n",
        "start_date = '2020-03-09'\n",
        "end_date = '2021-01-24'\n",
        "city = 'FLORIANOPOLIS'\n",
        "file = 'ftp://boavista:dados_abertos@ftp2.ciasc.gov.br/boavista_covid_dados_abertos.csv'\n",
        "file_mun = '/content/drive/My Drive/deepLearning/databases/municipio_populacao.csv'\n",
        "steps_in = 7\n",
        "steps_out = 14\n",
        "split_size = 0.7\n",
        "epochs = 300\n",
        "batch = 8\n",
        "nodes = 200\n",
        "\n",
        "# Lê o arquivo do banco de dados, ajusta das datas e nomina as colunas\n",
        "dataset = pd.read_csv(file, sep = ';',\n",
        "                      usecols = ['obito',\n",
        "                                 'data_obito',\n",
        "                                 'data_resultado',\n",
        "                                 'municipio_notificacao',\n",
        "                                 'classificacao'])\n",
        "dataset['data_obito'] = pd.to_datetime(dataset['data_obito']).dt.date\n",
        "dataset['data_resultado'] = pd.to_datetime(dataset['data_resultado']).dt.date\n",
        "dataset.columns = ['obito',\n",
        "                   'data_obito',\n",
        "                   'data_resultado',\n",
        "                   'municipio',\n",
        "                   'classificacao']\n",
        "\n",
        "# Tabula os dados\n",
        "sel_condition = 'CONFIRMADO'\n",
        "pivot_arg = ['data_resultado',\n",
        "             'municipio',\n",
        "             'classificacao',\n",
        "             sel_condition]\n",
        "table = pivot_table(dataset, pivot_arg)\n",
        "\n",
        "# Seleciona os municípios catarinenses da origem do paciente\n",
        "mun = pd.read_csv(file_mun, sep = ',')\n",
        "table = table.loc[:, table.columns.isin(mun.municipio)]\n",
        "table_rolling = table.rolling(window = 7).mean()\n",
        "\n",
        "# Limita período dos dados\n",
        "table_filtered = table[(table.index >= start_date) & (table.index <= end_date)]\n",
        "table_ma = table_rolling[(table_rolling.index >= start_date) &\n",
        "                         (table_rolling.index <= end_date)]\n",
        "\n",
        "# Visualização dos dados: evolução dos casos\n",
        "plt.figure(figsize = (18, 9))\n",
        "plt.plot(np.sum(table_ma.transpose()), label = 'Média móvel', color = 'r')\n",
        "plt.plot(np.sum(table_filtered.transpose()), label = 'Incidência diária',\n",
        "         color = 'b')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.savefig('inicidence.jpg', dpi = 600)\n",
        "\n",
        "# Visualização dos dados: Autocorrelação parcial e total\n",
        "plot_acf(np.sum(table_filtered, axis = 1), lags = 14)\n",
        "plt.savefig('acf.jpg', dpi = 600)\n",
        "plot_pacf(np.sum(table_filtered, axis = 1), lags = 14)\n",
        "plt.savefig('pacf.jpg', dpi = 600)\n",
        "\n",
        "# Divide dados de treino e teste\n",
        "train, test = train_test_split(table_filtered, split_size)\n",
        "X_train, y_train = split_sequences(train.values, steps_in, steps_out)\n",
        "X_test, y_test = split_sequences(test.values, steps_in, steps_out)\n",
        "if (X_test.size == 0 or y_test.size == 0): raise Exception('Not enough data!')\n",
        "\n",
        "# Constroi o modelo\n",
        "features = X_train.shape[2]\n",
        "model_arg = [epochs, batch, features, steps_in, steps_out, nodes]\n",
        "model = build_model(X_train, y_train, model_arg)\n",
        "\n",
        "# Faz as predições de teste\n",
        "test_predictions = model.predict(X_test);\n",
        "\n",
        "# Avalia o modelo em cada tempo de predição\n",
        "error_metric = []\n",
        "for i in range(test_predictions.shape[1]):\n",
        "    rmse = sqrt(mean_squared_error(y_test[:, i, :], test_predictions[:, i, :]))\n",
        "    error_metric.append(rmse)\n",
        "\n",
        "pd.DataFrame(error_metric).to_csv('error_metric.csv')\n",
        "wl = len(error_metric)\n",
        "\n",
        "# Calcula trendline\n",
        "z = np.polyfit(np.arange(1, (wl + 1)), error_metric, 1)\n",
        "p = np.poly1d(z)\n",
        "\n",
        "# Plota o gráfico\n",
        "plt.figure(figsize = (16, 8))\n",
        "plt.plot(p(np.arange(1, (wl + 1))), 'r--', label = 'Linha de Tendência')\n",
        "plt.plot(error_metric, label = 'RMSE')\n",
        "plt.xticks(np.arange(wl), np.arange(1, (wl + 1)).astype(str))\n",
        "plt.legend(loc = 'upper left')\n",
        "for i in range(wl):\n",
        "    plt.annotate(round(error_metric[i], 2), (i, error_metric[i]),\n",
        "                 textcoords = 'offset points', xytext = (-1, 10), ha = 'center')\n",
        "plt.savefig('error.jpg', dpi = 600)\n",
        "\n",
        "# Avalia o resultado geral do modelo\n",
        "rmse = sqrt(mean_squared_error(y_test.flatten(), test_predictions.flatten()))\n",
        "print('RMSE do modelo construído: {:.2f}'.format(rmse))\n",
        "\n",
        "# Plota cada dia do horizonte em relação aos valores reais\n",
        "n_plots = test_predictions.shape[1]\n",
        "plt.figure(figsize = (20, 28))\n",
        "for i in range(n_plots):\n",
        "    plot_pred = np.sum(test_predictions[:, i, :], axis = 1)\n",
        "    plot_y = np.sum(y_test[:, i, :], axis = 1)\n",
        "    plt.subplot(7, 3, (i + 1))\n",
        "    plt.plot(plot_y, label = 'Valores reais', color = 'r')\n",
        "    plt.plot(plot_pred, label = 'Valores preditos', color = 'b')\n",
        "    plt.title('t + ' + str(i + 1))\n",
        "    plt.legend(loc = 'upper left')\n",
        "    plt.subplots_adjust(hspace = 0.4)\n",
        "plt.savefig('horizon.jpg', dpi = 600)\n",
        "\n",
        "# Constrói predição futura\n",
        "prediction = model.predict(table_filtered.iloc[-7:].values.reshape(1, 7, -1))[0]\n",
        "datas = pd.date_range(start = table_filtered.index.max(),\n",
        "                      periods = steps_out + 1)[1:]\n",
        "predictions = pd.DataFrame(np.round(prediction, 0), index = datas,\n",
        "                           columns = table_filtered.columns).astype(int)\n",
        "\n",
        "table_case = table_filtered.append(predictions)\n",
        "table_case_ma = table_case.rolling(window = 7).mean()\n",
        "table_city = table_case.loc[:, city]\n",
        "table_city_ma = table_city.rolling(window = 7).mean()\n",
        "\n",
        "predictions.to_csv('predictions.csv')\n",
        "predictions[city].to_csv('predictions_city.csv')\n",
        "\n",
        "# Visualização dos dados: predição futura\n",
        "plt.figure(figsize = (18, 9))\n",
        "plt.plot(np.sum(table_case.transpose()),\n",
        "         label = 'Incidência Diária (valores reais)', color = 'b')\n",
        "plt.plot(np.sum(table_case_ma.transpose()),\n",
        "         label = 'Média Móvel (valores reais)', color = 'r')\n",
        "plt.plot(np.sum(table_case[-steps_out:].transpose()),\n",
        "         label = 'Incidência Diária (valores preditos)', color = 'g')\n",
        "plt.plot(np.sum(table_case_ma[-steps_out:].transpose()),\n",
        "         label = 'Média Móvel (valores preditos)', color = 'orange')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.vlines(table_filtered.index.max(), np.sum(table_case.transpose()).min(),\n",
        "           np.sum(table_case.transpose()).max(), linestyles = 'dotted')\n",
        "plt.text(table_filtered.index.max(), np.sum(table_case.transpose()).min(),\n",
        "         table_filtered.index.max().strftime('%d/%m/%Y'))\n",
        "plt.savefig('predictions.jpg', dpi = 600)\n",
        "\n",
        "# Visualização dos dados: predição futura município selecionado\n",
        "plt.figure(figsize = (18, 9))\n",
        "plt.plot([], [], ' ', label = city)\n",
        "plt.plot(table_city.transpose(),\n",
        "         label = 'Incidência Diária (valores reais)', color = 'b')\n",
        "plt.plot(table_city_ma.transpose(),\n",
        "         label = 'Média Móvel (valores reais)', color = 'r')\n",
        "plt.plot(table_city[-steps_out:].transpose(),\n",
        "         label = 'Incidência Diária (valores preditos)', color = 'g')\n",
        "plt.plot(table_city_ma[-steps_out:].transpose(),\n",
        "         label = 'Média Móvel (valores preditos)', color = 'orange')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.vlines(table_filtered.index.max(), table_city.min(),\n",
        "           table_city.max(), linestyles = 'dotted')\n",
        "plt.text(table_filtered.index.max(), table_city.min(),\n",
        "         table_filtered.index.max().strftime('%Y-%m-%d'))\n",
        "plt.savefig('predictions_city.jpg', dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}